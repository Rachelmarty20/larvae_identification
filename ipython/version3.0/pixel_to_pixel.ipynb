{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: FutureWarning: IPython widgets are experimental and may change in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd8ab5d1190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import mahotas as mh\n",
    "from wand.image import Image as WImage\n",
    "from wand.display import display\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # 10 x 8 inches\n",
    "plt.gray()\n",
    "import seaborn as sns\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import images with appropriate structure\n",
    "2. Build Layers\n",
    "3. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://cs231n.github.io/convolutional-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Import images with appropriate structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Might need to build a baby library of examples? These would be a slightly larger size than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "species = 'cfellah'\n",
    "size = 100\n",
    "directory = '/cellar/users/ramarty/Data/ants/version3.0/training/{0}/{1}'.format(species, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[90, 94, 89, ..., 73, 77, 79],\n",
       "       [92, 92, 93, ..., 64, 69, 72],\n",
       "       [94, 94, 92, ..., 63, 62, 65],\n",
       "       ..., \n",
       "       [66, 67, 67, ..., 54, 54, 51],\n",
       "       [68, 67, 68, ..., 53, 54, 52],\n",
       "       [67, 67, 68, ..., 56, 54, 55]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image\n",
    "mh.imread('{0}/images/{1}.pgm'.format(directory, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target\n",
    "np.load('{0}/targets/{1}.npy'.format(directory, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(number_of_images):\n",
    "    features = []\n",
    "    for i in range(number_of_images):\n",
    "        features.append(mh.imread('{0}/images/{1}.pgm'.format(directory, i)))\n",
    "    return np.array(features)\n",
    "def get_targets(number_of_targets):\n",
    "    targets = []\n",
    "    for i in range(number_of_targets):\n",
    "        targets.append(np.load('{0}/targets/{1}.npy'.format(directory, i)))\n",
    "    return np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = get_features(14000)\n",
    "y = get_targets(14000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and test - maybe use a dictionary like the example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [(X_train, y_train),  (X_test, y_test)]\n",
    "subsets = ['train', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "for (subset_data, subset_labels), subset_name in zip(data, subsets):\n",
    "    # The data is provided in the shape (n_examples, 784)\n",
    "    # where 784 = width*height = 28*28\n",
    "    # We need to reshape for convolutional layer shape conventions - explained below!\n",
    "    subset_data = subset_data.reshape(\n",
    "    (subset_data.shape[0], 1, 100, 100))\n",
    "    dataset[subset_name] = {\n",
    "    # We need to use data matrices of dtype theano.config.floatX\n",
    "    'X': subset_data.astype(theano.config.floatX),\n",
    "    # Labels are integers\n",
    "    'y': subset_labels.astype(theano.config.floatX)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Build layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input\n",
    "input_shape = dataset['train']['X'][0].shape\n",
    "# the first element is the batch size\n",
    "l_in = lasagne.layers.InputLayer(\n",
    "    shape=(None, input_shape[0], input_shape[1], input_shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1, 100, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convolution + pooling\n",
    "\n",
    "l_conv = lasagne.layers.Conv2DLayer(\n",
    "     l_in,\n",
    "     # Here, we set the number of filters and their size.\n",
    "     num_filters=50, filter_size=(5, 5),\n",
    "     # lasagne.nonlinearities.rectify is the common ReLU nonlinearity\n",
    "     nonlinearity=lasagne.nonlinearities.rectify,\n",
    "     # Use He et. al.'s initialization\n",
    "     W=lasagne.init.HeNormal(gain='relu'))\n",
    "    # Other arguments: Convolution type (full, same, or valid) and stride\n",
    "\n",
    "# Here, we do 2x2 max pooling. The max pooling layer also supports striding\n",
    "#l_pool1 = lasagne.layers.MaxPool2DLayer(l_conv1, pool_size=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dense layer - NINLayer??\n",
    "#l_dense1 = lasagne.layers.NINLayer(l_conv, num_units=5, W=lasagne.init.GlorotUniform('relu'), b=lasagne.init.Normal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TransposedConv2DLayer (also known as fractionally strided convolutions)\n",
    "# is this different after the pooling? Do I have to do an unpooling step???\n",
    "\n",
    "l_deconv = lasagne.layers.TransposedConv2DLayer(l_conv, num_filters=l_conv.input_shape[1], \n",
    "                                                 filter_size=l_conv.filter_size,\n",
    "                                                 nonlinearity=lasagne.nonlinearities.rectify)\n",
    "\n",
    "\n",
    "#Upscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output\n",
    "l_output = lasagne.layers.NINLayer(\n",
    "     l_deconv,\n",
    "     # The number of units in the softmas output layer is the number of classes.\n",
    "     num_units=1,\n",
    "     nonlinearity=lasagne.nonlinearities.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 100, 100)\n",
      "(None, 1, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "print l_output.input_shape\n",
    "print l_output.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = lasagne.layers.get_output(l_output)\n",
    "targets = T.tensor3('true_output') # unsure if this is right\n",
    "loss = lasagne.objectives.squared_error(predictions, targets)\n",
    "loss = lasagne.objectives.aggregate(loss, mode='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_params = lasagne.layers.get_all_params(l_output, trainable=True)\n",
    "updates = lasagne.updates.sgd(loss, all_params, learning_rate=0.001)\n",
    "train = theano.function([l_in.input_var, targets], loss, updates=updates, allow_input_downcast=True, name='Training')\n",
    "get_output = theano.function([l_in.input_var], predictions, name='get_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get_output = lasagne.layers.get_output(l_output, deterministic=True)\n",
    "#predict_fn = theano.function([l_in.input_var], T.argmax(get_output, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Test functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "Epoch 1 validation accuracy: 0.0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-a3d9ae36a96b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Extract the training data/label batch and update the parameters with it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     train(dataset['train']['X'][batch_idx:batch_idx + BATCH_SIZE],\n\u001b[1;32m---> 12\u001b[1;33m     dataset['train']['y'][batch_idx:batch_idx + BATCH_SIZE])\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mbatch_idx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Once we've trained on the entire training set...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/cellar/users/ramarty/anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now, let's train it! We'll chop the training data into mini-batches,\n",
    "# and compute the validation accuracy every epoch.\n",
    "BATCH_SIZE = 100\n",
    "N_EPOCHS = 50\n",
    "# Keep track of which batch we're training with\n",
    "batch_idx = 0\n",
    "# Keep track of which epoch we're on\n",
    "epoch = 0\n",
    "while epoch < N_EPOCHS:\n",
    "    # Extract the training data/label batch and update the parameters with it\n",
    "    train(dataset['train']['X'][batch_idx:batch_idx + BATCH_SIZE],\n",
    "    dataset['train']['y'][batch_idx:batch_idx + BATCH_SIZE])\n",
    "    batch_idx += BATCH_SIZE\n",
    "    # Once we've trained on the entire training set...\n",
    "    print batch_idx\n",
    "    if batch_idx >= dataset['train']['X'].shape[0]:\n",
    "        # Reset the batch index\n",
    "        batch_idx = 0\n",
    "        # Update the number of epochs trained\n",
    "        epoch += 1\n",
    "        # Compute the network's on the validation data\n",
    "        #val_output = get_output(dataset['valid']['X'])\n",
    "        # The predicted class is just the index of the largest probability in the output\n",
    "        #val_predictions = np.argmax(val_output, axis=1)\n",
    "        val_predictions = get_output(dataset['test']['X'])\n",
    "        # The accuracy is the average number of correct predictions\n",
    "        # TODO: this line doesn't do what we want... \n",
    "        accuracy = np.mean(val_predictions == dataset['test']['y'])\n",
    "        print(\"Epoch {} validation accuracy: {}\".format(epoch, accuracy))\n",
    "        # want to print precision and recall here instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4620"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['test']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "884.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['y'][0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test']['y'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "         1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "         1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n",
       "         1.        ,  1.        ],\n",
       "       ..., \n",
       "       [ 1.        ,  1.        ,  1.        , ...,  0.49996219,\n",
       "         1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ,  1.        , ...,  0.49996219,\n",
       "         0.98363334,  0.49996219],\n",
       "       [ 1.        ,  1.        ,  0.67586031, ...,  0.49996219,\n",
       "         0.49996219,  0.49996219]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_predictions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9949.15567501\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print val_predictions[0][0].sum() # the first value is the one that must be iterated!\n",
    "print val_predictions[0][0].max() # the first value is the one that must be iterated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cutoff = 0.25\n",
    "precisions, recalls = [], []\n",
    "for x in range(132):\n",
    "    TP, FP, FN = 0, 0, 0\n",
    "    for i in range(100):\n",
    "        for j in range(100):\n",
    "            # val_predictions should == 1 for TP and FP\n",
    "            if (dataset['test']['y'][x][i][j] > cutoff) & (val_predictions[x][0][i][j] > cutoff):\n",
    "                TP += 1\n",
    "            elif (dataset['test']['y'][x][i][j] <= cutoff) & (val_predictions[x][0][i][j] > cutoff):\n",
    "                FP += 1\n",
    "            elif (dataset['test']['y'][x][i][j] > cutoff) & (val_predictions[x][0][i][j] <= cutoff):\n",
    "                FN += 1\n",
    "    if TP + FP > 0: # & TP + FN > 0:\n",
    "        precision = TP/float(TP+FP)\n",
    "        recall = TP/float(TP+FN)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        print x, precision, recall, dataset['test']['y'][x].sum(), val_predictions[x][0].sum()\n",
    "        #print val_predictions[x][0].sum(), val_predictions[x][0].max(), val_predictions[x][0].min()\n",
    "    else:\n",
    "        print x, \"No positives\"\n",
    "        #print val_predictions[x][0].sum(), val_predictions[x][0].max(), val_predictions[x][0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No positives\n"
     ]
    }
   ],
   "source": [
    "if TP + FP > 0:\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    print precision, recall\n",
    "else:\n",
    "    print \"No positives\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Parameter sweep on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cluster_script(learning, batch):\n",
    "    \n",
    "    new_script_file = '/cellar/users/ramarty/Projects/ants/scripts/cluster/version3.0/parameter_sweep.v1.sh'\n",
    "    \n",
    "    with open(new_script_file, 'w') as out_file:\n",
    "        out_file.write(\"#! /bin/csh\\n\")\n",
    "        out_file.write(\"#$ -V\\n\")\n",
    "        out_file.write(\"#$ -S /bin/csh\\n\")\n",
    "        out_file.write(\"#$ -o /cellar/users/ramarty/Data/ants/sge-system_files/\\n\")\n",
    "        out_file.write(\"#$ -e /cellar/users/ramarty/Data/ants/sge-system_files/\\n\")\n",
    "        out_file.write(\"#$ -cwd\\n\")\n",
    "        out_file.write(\"#$ -t 1-{0}\\n\".format(len(learning)))\n",
    "        out_file.write(\"#$ -l h_vmem=16G\\n\")\n",
    "        out_file.write(\"#$ -tc 10\\n\")\n",
    "        out_file.write(\"#$ -l long\")\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "        out_file.write(\"set learnings=({0})\\n\".format(\" \".join([str(x) for x in learning])))\n",
    "        out_file.write(\"set batches=({0})\\n\".format(\" \".join([str(x) for x in batch])))\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "        out_file.write(\"set learning=$learnings[$SGE_TASK_ID]\\n\")\n",
    "        out_file.write(\"set batch=$batches[$SGE_TASK_ID]\\n\")\n",
    "        out_file.write(\"\\n\")\n",
    "\n",
    "        out_file.write(\"date\\n\")\n",
    "        out_file.write(\"hostname\\n\")\n",
    "        out_file.write(\"python /cellar/users/ramarty/Projects/ants/scripts/python/version3.0/parameter_sweep.v1.py $learning $batch\\n\")\n",
    "        out_file.write(\"date\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning = [0.001, 0.01, 0.1, 1, 10, 100, 1000]*5\n",
    "batch = [1]*5 + [20]*5 + [100]*5 + [500]*5 + [1000]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_cluster_script(learning, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
